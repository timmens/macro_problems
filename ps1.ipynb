{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "\n",
    "1. Find errors in data management\n",
    "2. Implement correct fixed effects iv regression\n",
    "3. Implement plotting code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro 2: Problem Sheet 1\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "> This notebook is structured as follows. First I define all necessary functions in a sort of abstract fashion. Then at the very end I call all functions to produce numerical results and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data/CNEF_PSID\")\n",
    "TEN_PERCENT_OF_ANNUAL_FULLTIME_HOURS = 208"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yearly_variables(year):\n",
    "    \"\"\"Returns list and dict of relevant columns given year.\n",
    "    \n",
    "    Convert variables names to year specific names. For example instead\n",
    "    of 'x11102' writes 'x1110285' for the year '85. Additionally add\n",
    "    variable name for individual id.\n",
    "    \n",
    "    For details on variable names and more information seek the\n",
    "    codebook: https://www.cnefdata.org/documentation/codebooks\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        year (int): Year. (Write 85 for 1985.)\n",
    "        \n",
    "    Returns:\n",
    "        as_list (list): List of variable names.\n",
    "        as_dict (dict): Dict of variable name translation to human\n",
    "            readable form.\n",
    "    \n",
    "    \"\"\"\n",
    "    variables = {\n",
    "        \"x11102\": \"household\",\n",
    "        \"i11102\": \"income\",\n",
    "        \"d11105\": \"relationship_to_head\",\n",
    "        \"d11101\": \"age\",\n",
    "        \"d11109\": \"education\",\n",
    "        \"e11101\": \"hours\",\n",
    "        \"e11102\": \"work\",\n",
    "    }\n",
    "    as_dict = {f\"{key}{year}\": value for key, value in variables.items()}\n",
    "    as_dict = dict(as_dict, **{\"x11101ll\": \"individual\"})\n",
    "    as_list = [f\"{key}{year}\" for key in variables.keys()]\n",
    "    as_list.append(\"x11101ll\")\n",
    "    return as_dict, as_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_given_year(year):\n",
    "    \"\"\"Load data and assign new columns given year.\n",
    "    \n",
    "    This already does some steps described in part 2 of exercise 1.\n",
    "    \n",
    "    Explanation of steps:\n",
    "    \n",
    "    1. Load data file given year\n",
    "    2. Rename columns\n",
    "    3. Set household and individual ID as (multi-)index\n",
    "    4. Drop all rows that have NaN on ALL columns\n",
    "    5. Assign / transform four variables:\n",
    "        1. year (the year)\n",
    "        3. relationship_to_head (remove whitespace)\n",
    "        4. work (remove whitespace)\n",
    "    6. Drop columns that are not needed anymore\n",
    "    7. Convert data types\n",
    "    \n",
    "    Args:\n",
    "        year (int): Year. (Write 85 for 1985.)\n",
    "        \n",
    "    Returns:\n",
    "        df (pd.DataFrame): Data frame with columss\n",
    "    \n",
    "    \"\"\"\n",
    "    cols_mapper, cols = get_yearly_variables(year)\n",
    "\n",
    "    df = pd.read_stata(DATA_PATH / f\"pequiv{year}.dta\", columns=cols)\n",
    "\n",
    "    df = df.rename(columns=cols_mapper)\n",
    "    df = df.set_index([\"household\", \"individual\"])\n",
    "    df = df.dropna(how=\"all\")\n",
    "    df = df.assign(\n",
    "        **{\n",
    "            \"year\": year,\n",
    "            \"relationship_to_head\": df.relationship_to_head.str.split(\" \").apply(\n",
    "                lambda s: s[0]\n",
    "            ),\n",
    "            \"work\": df.work.str[-1],\n",
    "        }\n",
    "    )\n",
    "    df = df.convert_dtypes()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Clean data frame.\n",
    "    \n",
    "    This does most steps described in part 2 of exercise 1.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Frame produced by :func:`load_data_given_year`.\n",
    "        \n",
    "    Returns:\n",
    "        df (pd.DataFrame): Cleaned data frame.\n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.query(\"relationship_to_head in ['head', 'partner']\")\n",
    "    df = df.assign(\n",
    "        **{\n",
    "            \"is_single\": df.relationship_to_head.groupby(\"household\").transform(\n",
    "                lambda x: set(x) != {\"head\", \"partner\"}\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    df = df.query(\"is_single == False\")\n",
    "\n",
    "    total = df.groupby(by=\"household\")[[\"hours\", \"income\"]].sum()\n",
    "\n",
    "    df = df.query(\"relationship_to_head == 'head'\")\n",
    "    df = df.reset_index(level=\"individual\", drop=True)\n",
    "    df = df.assign(**{\"income\": total.income, \"hours\": total.hours})\n",
    "\n",
    "    df = df.query(\"25 <= age < 56\")\n",
    "    df = df.query(\"hours >= @TEN_PERCENT_OF_ANNUAL_FULLTIME_HOURS\")\n",
    "    df = df.query(\"income > 0\")\n",
    "\n",
    "    df = df.assign(**{\"income\": np.log(df.income)})\n",
    "\n",
    "    df = df.drop([\"relationship_to_head\", \"is_single\", \"hours\"], axis=1)\n",
    "    df = df.astype(\n",
    "        {\n",
    "            \"income\": float,\n",
    "            \"education\": \"category\",\n",
    "            \"age\": \"category\",\n",
    "            \"year\": \"category\",\n",
    "            \"work\": \"category\",\n",
    "        }\n",
    "    )\n",
    "    df = df.set_index(\"age\", append=True)\n",
    "    df = df.dropna(how=\"any\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data():\n",
    "    \"\"\"Load, clean and merge data for years 1980 to 1997.\n",
    "    \n",
    "    Since loading and cleaning the data is time consuming there is\n",
    "    a check if the clean data is already available.\n",
    "    \n",
    "    Returns:\n",
    "        df (pd.DataFrame): Cleaned and merged data frame with index ['household', 'year']\n",
    "            and columns 'income', 'age', 'education' and 'work'. Column 'income' is float\n",
    "            while all other columns are category.\n",
    "    \n",
    "    \"\"\"\n",
    "    clean_data_path = DATA_PATH / \"clean_data.csv\"\n",
    "    if clean_data_path.exists():\n",
    "        df = pd.read_csv(clean_data_path)\n",
    "    else:\n",
    "        dfs = []\n",
    "        for year in range(80, 98):\n",
    "            df = load_data_given_year(year)\n",
    "            df = clean_data(df)\n",
    "            dfs.append(df)\n",
    "        df = pd.concat(dfs).sort_index().reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_dummy_regression(df):\n",
    "    \"\"\"Fit dummy regression on data in df.\n",
    "    \n",
    "    In the formula object C() tells statsmodels to use the variable as\n",
    "    categorical variable.\n",
    "    \n",
    "    \"\"\"\n",
    "    model = ols(\n",
    "        \"income ~ C(year) + C(age) + C(education) + C(work)\", data=df\n",
    "    )\n",
    "    model = model.fit()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_residuals_to_df(df, model):\n",
    "    \"\"\"Add column residuals to data frame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Data from :func:`load_and_clean_data()`.\n",
    "        model (statsmodels.model): Model fitted in :func:`fit_dummy_regression`.\n",
    "        \n",
    "    Returns:\n",
    "        df (pd.DataFrame): As initial df but with column residuals from model.\n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.assign(**{\"residuals\": model.resid})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_quantities_approach_4(df):\n",
    "    \"\"\"Estimate quantities using approach in part 4.\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "        quantities (pd.DataFrame): Estimates of rho, sigma_eps^2 and sigma_mu_tau^2.\n",
    "        var (pd.DataFrame): Sample variances.\n",
    "\n",
    "    \"\"\"\n",
    "    var = df.query(\"age in [25, 40, 55]\")[[\"age\", \"residuals\"]].groupby(\"age\").var()\n",
    "\n",
    "    rho = ((var.loc[55][0] - var.loc[40][0]) / (var.loc[40][0] - var.loc[25][0]))\n",
    "    rho = rho ** (1 / 30)\n",
    "    gamma = rho ** 2 * (1 - rho ** 30) / (1 - rho ** 2)\n",
    "\n",
    "    sigma_eps =  (var.loc[40][0] - var.loc[25][0]) / gamma\n",
    "    sigma_mu_tau = var.loc[25][0] - sigma_eps\n",
    "\n",
    "    quantities = pd.DataFrame(\n",
    "        [rho, sigma_eps, sigma_mu_tau],\n",
    "        columns=[\"value\"],\n",
    "        index=[\"$\\rho$\", \"$\\sigma_{\\epsilon}^2$\", \"$\\sigma_{\\mu\\tau}^2$\"],\n",
    "    )\n",
    "    return quantities, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_quantities_approach_5(df, var):\n",
    "    \"\"\"Estimate quantities using approach in part 5.\n",
    "    \n",
    "    \"\"\"\n",
    "    # compute (sample) covariances\n",
    "    combinations = [(40, 39), (40, 38), (40, 37)]\n",
    "    cov = pd.DataFrame(index=pd.MultiIndex.from_tuples(combinations))\n",
    "    for comb in combinations:\n",
    "\n",
    "        _df = df[[\"household\", \"age\", \"residuals\"]].query(\"age in @comb\")\n",
    "        idx = _df.groupby(\"household\")[\"age\"].transform(\n",
    "            lambda x:  set(x) == set(comb) and len(x) == 2\n",
    "        )\n",
    "        _df = _df.loc[idx, ].set_index([\"household\", \"age\"])\n",
    "\n",
    "        _cov = _df.unstack(level=\"age\").cov().values[0, 1]\n",
    "        cov.loc[comb, \"cov\"] = _cov\n",
    "\n",
    "    # compute quantities\n",
    "    rho = ((cov.loc[(40, 37),] - cov.loc[(40, 38)]) / (\n",
    "        cov.loc[(40, 38),] - cov.loc[(40, 39)]\n",
    "    ))[0]\n",
    "    \n",
    "    sigma_eps = (cov.loc[(40, 37), ] - cov.loc[(40, 37), ])[0] * (1 - rho ** 2) / (rho * (rho - 1) * (rho ** 29 + 1)) \n",
    "    sigma_mu = cov.loc[(40, 39), ][0] - sigma_eps * rho * (1 - rho ** 30) / (1 - rho ** 2)\n",
    "    sigma_tau = var.loc[25][0] - sigma_mu - sigma_eps\n",
    "    \n",
    "    quantities = pd.DataFrame(\n",
    "        [rho, sigma_eps, sigma_mu, sigma_tau],\n",
    "        columns=[\"value\"],\n",
    "        index=[\"$\\rho$\", \"$\\sigma_{\\epsilon}^2$\", \"$\\sigma_{\\mu}^2$\", \"$\\sigma_{\\tau}^2$\"],\n",
    "    )\n",
    "    return quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_rho_approach_iv(df):\n",
    "    \"\"\"TODO!!!\"\"\"\n",
    "    \n",
    "    # construct instrument\n",
    "    #for household, age in df.index:\n",
    "        # try:\n",
    "        #     instrument = df.loc[(household, age - 2), \"residual\"] - df.loc[(household, age - 2), \"residual\"]\n",
    "        # except:\n",
    "        #    instrument = np.nan\n",
    "        \n",
    "    #    try:\n",
    "    #        lagged_residual = df.loc[(household, age-1), \"residual\"]\n",
    "    #    except:\n",
    "    #        lagged_residual = np.nan\n",
    "            \n",
    "    #    df.loc[(household, age), \"instrument\"] = instrument\n",
    "    #    df.loc[(household, age), \"lagged_residual\"] = lagged_residual\n",
    "    \n",
    "    # df = df.dropna(how=\"any\")\n",
    "    \n",
    "    # from statsmodels.formula.api import mixedlm\n",
    "    # https://www.statsmodels.org/stable/mixed_linear.html\n",
    "    # model = mixedlm(\"residual ~ lagged_residual\", data=df, groups=df.index.get_level_values(\"household\"))\n",
    "    \n",
    "    # check out: https://bashtage.github.io/linearmodels/doc/index.html\n",
    "    quantity = pd.Series([0.5], index=[\"$\\rho$\"], name=\"value\").to_frame()\n",
    "    return quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_parts(part4, part5, part6):\n",
    "    keys = [\"part4\", \"part5\", \"part6\"]\n",
    "    df = pd.concat((part4, part5, part6), axis=1, keys=keys)\n",
    "    df = df.droplevel(level=1, axis=1)\n",
    "    df = df.convert_dtypes()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_dummy_regression(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_residuals_to_df(df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "part4, var = estimate_quantities_approach_4(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "part5 = estimate_quantities_approach_5(df, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "part6 = estimate_rho_approach_iv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = combine_parts(part4, part5, part6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>part4</th>\n",
       "      <th>part5</th>\n",
       "      <th>part6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>$\\rho$</th>\n",
       "      <td>1.023501</td>\n",
       "      <td>1.568958</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$\\sigma_{\\epsilon}^2$</th>\n",
       "      <td>0.00116</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$\\sigma_{\\mu\\tau}^2$</th>\n",
       "      <td>0.194442</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$\\sigma_{\\mu}^2$</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.01292</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$\\sigma_{\\tau}^2$</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.182683</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          part4     part5  part6\n",
       "$\\rho$                 1.023501  1.568958    0.5\n",
       "$\\sigma_{\\epsilon}^2$   0.00116      -0.0   <NA>\n",
       "$\\sigma_{\\mu\\tau}^2$   0.194442      <NA>   <NA>\n",
       "$\\sigma_{\\mu}^2$           <NA>   0.01292   <NA>\n",
       "$\\sigma_{\\tau}^2$          <NA>  0.182683   <NA>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
